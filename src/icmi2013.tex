% This is "sig-alternate.tex" V2.0 May 2012
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate-2013}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\usepackage{subfigure}
\usepackage{textcomp}
\usepackage{tikz}
\usetikzlibrary{arrows,positioning} 

% Redefines \@ptsize to make setspace happy
% \makeatletter
% \renewcommand{\@ptsize}{0}
% \makeatother

%Double-spaces the entire document
% \usepackage{setspace}
% \doublespacing

\newfont{\mycrnotice}{ptmr8t at 7pt}
\newfont{\myconfname}{ptmri8t at 7pt}
\let\crnotice\mycrnotice%
\let\confname\myconfname%

\permission{Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.}
\conferenceinfo{ICMI'13,}{December 9--13, 2013, Sydney, Australia \\
{\mycrnotice{Copyright is held by the owner/author(s). Publication rights licensed to ACM.}}}
\copyrightetc{ACM \the\acmcopyr}
\crdata{978-1-4503-2129-7/13/12\ ...\$15.00.\\http://dx.doi.org/10.1145/2522848.2532588}

\clubpenalty=10000
\widowpenalty=10000

\begin{document}
%
% --- Author Metadata here ---
% IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Gesture Spotting and Recognition Using Salience
Detection and Concatenated Hidden Markov Models}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{2} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Ying Yin\\
       \affaddr{Massachusetts Institute of Technology}\\
       \affaddr{}\\
       \affaddr{}\\
       \email{yingyin@csail.mit.edu}
% 2nd. author
\alignauthor
Randall Davis \\
       \affaddr{Massachusetts Institute of Technology}\\
       \affaddr{}\\
       \affaddr{}\\
       \email{davis@csail.mit.edu}
}

\maketitle
\begin{abstract}
We developed a gesture salience based hand tracking method, and a gesture
spotting and recognition method based on concatenated hidden Markov models.
A 3-fold cross validation using the ChAirGest development data set with 10 users gives an F1 score of 0.907
and an accurate temporal segmentation rate (ATSR) of 0.923. The average final
score is 0.9116. Compared with using the hand joint position from the Kinect
SDK, using our hand tracking method gives a 3.7\% absolute increase in the
recognition F1 score.
\end{abstract}

%A category including the fourth, optional field follows...
\category{H.5.2}{Information Systems}{Information Interfaces and
Presentation}[User Interfaces]

\keywords{Gesture recognition; HMM; gesture spotting; Kinect}

\section{Introduction}
With the introduction of relatively low-cost sensors for tracking body or hand
movement, such as Microsoft's Kinect and the Leap Motion sensor, we are observing 
an increasing interest in using these sensors for natural human computer
interaction.
Gesture input is a main part of natural interaction, and as a result, improving the accuracy of
continuous gesture spotting and recognition remains an important topic.

We present our approach to the gesture spotting and recognition problem,
highlighting two main contributions:
improved hand position detection based on gesture salience and gesture spotting
using concatenated hidden Markov models (HMMs).

Our work builds on several related works. Marcos-Ramiro et al.~\cite{marcos2013} developed a method of computing hand likelihood maps based on RGB videos. 
Our hand tracking method is similar, but we use both RGB and depth data. 
In common with~\cite{Starner95, yin10}, we use hidden Markov models (HMMs) to model dynamic gestures,
but we train models separately for the pre-stroke, nucleus and post-stroke phases, concatenate them together and use Viterbi decoding to optimally segment the
gesture sequences. 

In the following sections, we explain our system in three main parts: feature extraction, temporal segmentation, and gesture
spotting and recognition.  

\section{Hand Feature Extraction}
We use both the Kinect and the Xsens data from the ChAirGest corpus~\cite{Ruffieux2013} to
extract hand motion feature vectors for gesture modeling.
It is relatively easy to obtain features from the Xsens data. We choose to use linear
acceleration (x, y, z), angular velocity (x, y, z) and Euler orientation (yaw, pitch, roll)
from the Xsens unit on the hand to form a 9-dimensional feature vector $\underline{x}_t^{\text{xsens}}$
for every time frame $t$.
From the Kinect sensor, we extract the position of the gesturing hand in (x, y, z) relative to
the shoulder center joint to
form a 3-dimensional vector $\underline{x}_t^{\text{kinect}}$. Combining the two, we
have a 12-dimensional feature vector $\underline{x}_t = [\underline{x}^\text{kinect}_t, \underline{x}^\text{xsens}_t]$.

We use the Kinect skeleton tracking result for the shoulder center joint position,
but do not use it for the hand position because
it is not accurate when the hands are close to the body or when the hands are moving fast.
We developed an improved method for hand tracking based on gesture salience using both
RGB and depth data.

\subsection{Gesture Salience Detection}
Similar to Marcos-Ramiro et al.~\cite{marcos2013}, we define gesture
\textit{salience} as a function of both the closeness of the motion to the
observer (e.g., the camera) and the magnitude of the motion.
There are 4 steps in our method (Figure~\ref{fig:gesture-salience}). 

\begin{figure*}[tb]
\centering
\hspace{-0.6em}%
\subfigure[]{
\includegraphics[width=0.19\linewidth]{fig/color.eps}\hspace{-0.6em}%
\label{fig:color}
}
\subfigure[]{
\includegraphics[width=0.19\linewidth]{fig/depth.eps}\hspace{-0.6em}
\label{fig:skin-mask}
}
\subfigure[]{
\includegraphics[width=0.19\linewidth]{fig/motion-mask1.eps}\hspace{-0.6em}
\label{fig:motion-mask}
}
\subfigure[]{
\includegraphics[width=0.19\linewidth]{fig/salient-map.eps}\hspace{-0.6em}
\label{fig:salience}
}
\subfigure[]{
\includegraphics[width=0.19\linewidth]{fig/bounding-box.eps}
\label{fig:camshift}
}
\caption{Gesture salience detection steps: \subref{fig:color} RGB image under low lighting condition;
\subref{fig:skin-mask} depth map $D_t$ filtered by skin and user mask, $M_t^{S\wedge U}$. False detection of skin is due to
the similar colors between clothes and skin; \subref{fig:motion-mask} motion mask,  $M_{t\vee t-1}^M$, indicating moved pixels for time $t$ and $t-1$;
\subref{fig:salience} salience map with red color indicating high probability of the salience; 
\subref{fig:camshift} final gesture salience bounding box, $B_t$. (Best viewed in
color. Based on data from ChAirGest corpus~\cite{Ruffieux2013}.)}
\label{fig:gesture-salience}
\end{figure*}

\subsubsection{Skin Segmentation}
We use an off-the-shelf simple skin color detection method to compute a binary skin mask at
time $t$, $M_t^S$, based on the RGB image. We also find the user mask, $M_t^U$ obtained from the Kinect SDK based on the depth image. 
We align the two to find their intersection $M_t^{S\wedge U}$, which indicates the user's skin region.

\subsubsection{Motion Detection}
We compute the motion mask for the current depth frame based on 3 frames. We first filter each 
depth frame by the user and skin mask $M_t^{S\wedge U}$, and then
smooth it through a median filter to obtain $D_t$ (Figure~\ref{fig:skin-mask}).
Equation~(\ref{eq:motion-mask}) computes the binary mask, $M_{t\vee t-1}^M$, 
indicating pixels whose depth values have changed from time $t-1$ to $t$ (Figure~\ref{fig:motion-mask}).
$D_{t\vee t-1}$ is the absolute difference between $D_t$ and $D_{t-1}$, and $T$ is the threshold operator that filters out small changes in depth value 
(with a threshold of 15mm). 
To obtain the motion mask, $M_{t}^M$ for time $t$ only, we use $M_{t-1\vee t-2}^M$, the motion mask for $t-1$ and $t-2$ as well (see Equation~(\ref{eq:motion-mask-t}),
 AND and XOR are indicated by $\wedge$ and $\oplus$).
\begin{align}
M_{t\vee t-1}^M &= T(D_{t\vee t-1}) \label{eq:motion-mask} \\
M_{t}^M &= M_{t\vee t-1}^M \oplus (M_{t\vee t-1}^M \wedge M_{t-1\vee t-2}^M) \label{eq:motion-mask-t}
\end{align}

\subsubsection{Salience Map}
We compute histograms of depth values in both $D_t$ and $D_{t\vee t-1}$ and then apply histogram normalization to obtain cumulative distributions $H_t$ and $H_{t\vee t-1}$.
$H_t$ represents the probability of salience given a depth value, while $H_{t\vee t-1}$ represents the probability of salience given
a depth difference value. The lower the depth value or the higher the depth difference value, the higher the salience probability. We use
histogram equalization to reduce the effect of outliers, so that a single large depth value will not suppress the salience probabilities of other depth values. 
The salience map (Figure~\ref{fig:salience}) can then be computed for each pixel $(x, y)$:
\begin{align*}
S_t(x, y) = H_t(D_t(x, y)) \times H_{t\vee t-1}(D_{t\vee t-1}(x, y)) \times M_t^M
\end{align*}
The multiplication of the binary motion mask $M_t^M$ allows us to consider only the motion due to the user at $t$.
 
\subsubsection{Salience Location}
The final step of locating the most salient region in a frame is finding the
contour, $C_t$, from the salience map $S_t$ that has a perimeter greater than
the smallest possible hand perimeter and with the highest average salience for all the pixels inside the contour.

When motion is slow, the motion mask usually indicates the edge of the moving
object. As a result, the center of $C_t$ may not be the center of the moving
object (in our case, the user's hand). Hence, we use 2 iterations of Camshift~\cite{Bradski98} on 
the depth image $D_t$ with a starting search location at the center of $C_t$ to refine
the final bounding box, $B_t$, of gesture salience (Figure~\ref{fig:camshift}). 

Figure~\ref{fig:compare-skeleton} shows examples of our hand tracking result (red regions). 
It is more reliable than the hand joint locations from the Kinect SDK. In the
Experimental Evaluation section (Section~\ref{sec:eval}), we show that using our 
salience detection method to extract hand position features gives 3.7\%
absolute increase in gesture recognition F1 score compared to using the hand
joint position from the Kinect SDK.

\begin{figure*}
\centering
\subfigure[]{
\includegraphics[width=0.23\linewidth]{fig/rotate-color.eps} \hspace{-0.6em}
}
\subfigure[]{
\includegraphics[width=0.23\linewidth]{fig/rotate-depth.eps} \hspace{-0.6em}
}
\subfigure[]{
\includegraphics[width=0.23\linewidth]{fig/near-body-color.eps}
\hspace{-0.6em} }
\subfigure[]{
\includegraphics[width=0.23\linewidth]{fig/near-body-depth.eps}
\hspace{-0.6em} }
\caption{Comparison of hand tracking results. Our method (red region) gives more
reliable result on hand tracking compared to the off-the-shelf Kinect software
(green line). (Best viewed in color. Based on data from ChAirGest
corpus~\cite{Ruffieux2013}.)}
\label{fig:compare-skeleton}
\end{figure*}

\vfill

\section{Temporal Segmentation}\label{sec:recognition}
We used all the training data from all the users to create a Gaussian model for the rest 
positions and a Gaussian model for non-rest positions.

During recognition, an observation $\underline{x}_t$ is first classified as a 
rest or a non-rest position. It is a non-rest position if 
\begin{displaymath}
N(\underline{x}_t; \mu_{\text{NON-REST}}, \Sigma_{\text{NON-REST}}) \geq N(\underline{x}_t; \mu_{\text{REST}}, \Sigma_{\text{REST}})
\end{displaymath}
where $N$ represents the Gaussian probability. Sequences of continuous observations from non-rest
positions longer than $d$ seconds are further classified into different
gestures based on trained HMMs. The threshold value $d$ is the lower bound of
gesture duration, and can be varied for different gesture sets. We set it to
be 0.25s for our evaluation data set based on empirical result.
 
\section{Gesture Spotting and Recognition}
\subsection{Temporal Gesture Modeling and Training}
Previous research suggests that
a gesture consists of three phases: \textit{pre-stroke}, \textit{nucleus}, and \textit{post-stroke}~\cite{Pavlovic97}. The pre-stroke phase consists
of a preparatory movement that sets the hand in motion from some resting position.
The nucleus of a gesture has some ``definite form and enhanced dynamic qualities''
~\cite{kendon86}. Finally, the hand either returns to the resting position or repositions
for the new gesture phase. Each gesture
phase includes a sequence of hand/arm movement that can be modeled using HMMs (see Figure~\ref{fig:hmm}). 

\begin{figure}[tb]
\centering
\includegraphics[clip, width=1\columnwidth]{fig/hmm.ps}
\caption{Temporal gesture model with different phases. Each phase can be modeled as an HMM. Dashed arrows represent
initial state transitions and double circles
represent end states.}
\label{fig:hmm}
\end{figure}

Because we have the ground truth labeling of pre-stroke, nucleus and post-stroke phases, 
we can train an HMM for each phase for each gesture. 

As each phase can have variable length, we model the termination probability for each
hidden state $s$ as $t(\text{END}|s)$. Given a sequence of observation $\underline{X}_1^T = \underline{x}_1\ldots\underline{x}_T$, and 
the corresponding hidden states sequence $S_1^T = s_1\ldots s_T$, we define the probability
\begin{displaymath}
p(\underline{X}_1^T, S_1^T;\underline{\theta}) = 
    t(s_1)t(END|s_T)\prod_{t = 2}^T t(s_t | s_{t-1})\prod_{t = 1}^T e(\underline{x}_t|s_t)
\end{displaymath}
where $\underline{\theta}$ represents the model parameter vector which includes
the initial state probabilities $t(s)$, the state transition probabilities $t(s'|s)$, and the 
emission probabilities $e(\underline{x}|s)$ for $s, s'\in \{1, 2,\ldots k\}$. 
We use a mixture of 6 Gaussians for the emission probability to model user variations.

Given $N$ training sequences, we use the expectation maximization (EM) algorithm to estimate the model parameters. In
particular, the update for the termination probability during the $i$th iteration is 
\begin{displaymath}
t^i(END|s) = \frac{\sum_{j = 1}^N \overline{count}(j, s\rightarrow END;\underline{\theta}^{i-1})}
    {\sum_{j = 1}^N\sum_{s'} \overline{count}(j, s\rightarrow s';\underline{\theta}^{i-1})}
\end{displaymath}
where $\overline{count}(j, s\rightarrow END;\underline{\theta}^{i-1})$ is the expected count of 
$s$ being the end state. We can use the usual forward-backward algorithm to compute all the 
expected sufficient statistics by adding a dummy END state to the end of each sequence.

Because there are 3 rest positions, we use 3 hidden states for both the pre-stroke and post-stroke phases.
Each hidden state can be the start state and can only remain in its own state or go to the end state.
 
For the nucleus phase, we use 6 hidden states (chosen through cross validation) for all the gestures and use a modified Bakis~\cite{bauer2000} model to constrain the transition probabilities
among the hidden states. Instead of allowing only left-right transition, we allow the last hidden state
to go back to the initial state (Figure~\ref{fig:bakis}). This is particularly important for modeling gestures with arbitrary number of
repetitions such as waving and shaking hands. 

\tikzstyle{vertex}=[circle, draw, minimum size=16pt, inner sep=0pt]
\tikzstyle{observed-vertex}=[circle, draw, minimum size=16pt, inner
sep=0pt, fill=black!20] 
\tikzstyle{edge} = [draw, thick, -]
\tikzstyle{directed-edge} = [draw, ->]

\begin{figure}[tb]
\centering
  \begin{tikzpicture}[auto,swap, scale=1.5]
    % First we draw the vertices
    \foreach \pos/\name in {{(0, 0)/start}, {(1, 0)/s_1}, {(2, 0)/s_2},
    {(3, 0)/s_3}, {(4, 0)/s_4}, {(5, 0)/end}}
      \node[vertex] (\name) at \pos {$\name$};
    % Connect vertices with edges and draw weights
    \foreach \source/ \dest in {s_2/s_3, s_3/s_4, s_4/end,
    s_1/s_2} \path[directed-edge] (\source) -- (\dest);
    
    \foreach \source/ \dest in {s_1/s_3, start/s_2, s_2/s_4, s_3/end, s_4/s_1} 
      \path[directed-edge] (\source) edge [bend left] (\dest);
      
    \foreach \source/ \dest in {s_1/s_1, s_2/s_2, s_3/s_3, s_4/s_4} 
      \path[directed-edge] (\source) edge [loop above] (\dest);
    
    \path[directed-edge] (start) edge node [below] {$t(s_1)$} (s_1);
  \end{tikzpicture}
  \caption{A state transition diagram of a modified 4-state Bakis model for the nucleus phase.}
  \label{fig:bakis}
\end{figure}

\subsection{Gesture Recognition}
During the recognition phase, we concatenate the HMMs trained for each phase together to form
one HMM for each gesture. The transition probability from the previous phase to the next
phase can be computed by multiplying the termination probabilities of the previous phase and the
initial state probabilities of the next phase. Using the superscript $c$ to denote the model
parameters in the concatenated HMM, we have
\begin{displaymath}
t^c(s_\text{nucleus}|s_\text{prestroke}) = t(\text{END}|s_\text{prestroke}) \times t(s_\text{nucleus})
\end{displaymath}
where $s_{\text{phase}}$ denotes the hidden state variable in a particular phase. We add small transition
probabilities from the pre-stroke phase to the post-stroke phase to model movements that do not have the 
nucleus phase.

As new state transition probabilities are added, the transition probabilities among
states in the same phase also need to be modified so that $\sum_{s' = 1}^K t(s'|s) = 1$ (where
$K$ is the total number of combined hidden states) is ensured. For example
\begin{displaymath}
t^c(s'_{\text{nucleus}} | s_{\text{nucleus}}) = t(s'_{\text{nucleus}} | s_{\text{nucleus}})
  \times (1 - t(\text{END} | s_{\text{nucleus}}))
\end{displaymath}

We also add a rest state to the end of the HMM and allow the rest state to transit to the pre-stroke
and the post-stroke phase with uniform probabilities (Figure~\ref{fig:hmm}) to accommodate short pauses during the gesture.
As a result, the final HMM for each gesture has 13 hidden states.
Let $\underline{\theta}_g$ be the final concatenated HMM parameters for gesture $g$. The classification
of an observation sequence from non-rest positions is 
\begin{displaymath}
\hat{g} = \arg\max_g\log p(\underline{X}_1^T; \underline{\theta}_g)
\end{displaymath}

\subsection{Gesture Spotting}
Because the non-rest positions include both pre-stroke and post-stroke phases, we need
to detect the start of the actual gesture (nucleus). We use the Viterbi algorithm
to find the most probable hidden state sequence $\hat{s}_1\ldots\hat{s}_T$ for a given observed sequence using 
the mostly likely gesture model $\underline{\theta}_{\hat{g}}$. The start and the end time for a gesture nucleus are
the first and the last time frame $t$ where $\hat{s}_t\in s_{\text{nucleus}}$ respectively. Note that
we are able to identify whether a hidden state belongs to the nucleus phase because we trained the three phases
separately.

\begin{figure}[tb]
\centering
\includegraphics[trim={4cm 1cm 0cm 0.6cm}, clip,
width=1\columnwidth]{fig/gesture.eps} \caption{Visualization of a gesture
recognition sequence.
The pre-stroke and post-stroke phases are indicated by two orange colors (see the color bar).}
\label{fig:decoding}
\end{figure}

\begin{figure}[tb]
\centering
\includegraphics[trim={3.5cm 0cm 0.5cm 0cm}, clip,
width=1\columnwidth]{fig/hiddenstates.eps} \caption{Visualization of the most
probable hidden states of a gesture recognition sequence.
Colors 1-3 indicate the pre-stroke hidden states, colors 4 - 9
indicate the nucleus hidden states, colors 10 - 12 indicate the post-stroke
hidden states, and color 14 indicates the rest state.}
\label{fig:hiddenstates}
\end{figure}

Figure~\ref{fig:decoding} shows a recognition result visualization for one batch sequence. The first
row is the ground truth with different colors indicating different gesture phases or the rest position. 
The second row is our segmentation and recognition result.
Figure~\ref{fig:hiddenstates} shows the color-coded most probable hidden states
for the same sequence.
If a non-rest sequence does not contain hidden states belonging to the nucleus
phase, it is ignored (see the blue bar at $t\sim 30700$ in Figure~\ref{fig:hiddenstates}).
In this way, we can spot the actual gestures while filtering out other movements.

\section{Experimental Evaluation}\label{sec:eval}
We evaluate our method based on the development data set from the ChAirGest
corpus~\cite{Ruffieux2013} with gestures captured from 10 users. There are 900
total gesture occurrences (3 recording sessions for each user) in the
development data set representing three forth of the entire corpus. The remaining one forth of the corpus are not released to
the public, and can be used for final evaluation on unseen data.

\begin{table}[tb]
\begin{center}
\begin{tabular}{|l|p{2cm}|p{1.7cm}|p{1.7cm}|}
\hline
 & Hand position from salience detection \& Xsens & Hand position
 from Kinect skeleton \& Xsens & Xsens Only \\
\hline
F1 Score & \textbf{0.907 (0.01)} & 0.870 (0.02) & 0.890 (0.02) \\
\hline
ATSR Score & \textbf{0.923 (0.02)} & 0.930 (0.03) & 0.920 (0.01) \\
\hline
Final Score & \textbf{0.912 (0.01)} & 0.881 (0.01) & 0.895 (0.01) \\
\hline
\end{tabular}
\caption{Comparison of the average 3-fold cross validation results for different
feature vectors. Values in parentheses are standard deviations.}
\label{tab:comp-feature}
\end{center}
\end{table}

Table~\ref{tab:comp-feature} compares the gesture recognition performance using
different methods of computing feature vectors while keeping the recognition
method the same.
They are the average results of 3-fold cross validations where, in each fold, the model is trained on 2 sessions and tested on 1 session from every user.
The results help to answer the following questions:
\begin{enumerate}
  \item Does our salience based hand tracking method give better performance?
  The first column in Table~\ref{tab:comp-feature} shows the results from
  using the salience based method to compute relative hand positions, and the
  second column are the results from using the hand joint positions from the
  Kinect SDK's skeleton tracking (version 1.6). The salience base method gives
  3.7\% absolute increase in the F1 score.
  \item Does including the relative hand position in the feature vector help to
  increase performance? The third column shows the results from using only Xsens
  features. We observe that including the relative hand position computed
  using our hand tracking method gives 1.7\% absolute increase in the F1 score.
  However, using the hand positions from the Kinect SDK actually decreases the
  performance.
\end{enumerate}

The ATSR score in Table~\ref{tab:comp-feature} stands for the accurate temporal
segmentation rate which represents the performance of accurately
detecting the start and stop points of gesture events~\cite{Ruffieux2013}.
Let $e^\text{start}_i$ be the start error rate for a detected gesture nucleus
$i$, and
\begin{align*}
e^\text{start}_i = \frac{\text{ground truth start time} - \text{detected start
time}}{\text{ground truth duration of the gesture nucleus }i}
\end{align*}
The stop error rate, $e_i^\text{stop}$, is defined similarly.
Let $n_{\text{hit}}$ be the number correctly recognized gestures (i.e., the
label of the gesture nucleus $i$ is correct, and the absolute start error and
the stop error rates are less than 50\%). The formula for ATSR
is
\begin{align*}
ATSR = 1 - \frac{\sum_{i = 1}^{n_{\text{hit}}} |e^\text{start}_i| +
|e_i^\text{stop}|}{2\cdot n_{\text{hit}}}
\end{align*}

The per frame classification confusion matrix (Figure~\ref{fig:confusion}) shows that the most easily confused gestures are ``take from screen'' and ``push to screen''. These two gestures are very similar except for hand
poses. This suggest that by including hand pose features may further improve
the recognition accuracy. There are also more confusions at the boundary between phases, e.g., 
from rest to pre-stroke and from post-stroke to rest.

\begin{figure}[tb]
\centering
\includegraphics[trim={6cm 3.5cm 10cm 1.5cm}, clip, width=1\columnwidth]{fig/confusion-matrix.eps}
\caption{Per frame classification confusion matrix based on result from 3-fold cross validation using both Kinect and Xsens
features. The numbers are percentages. The darker the color the higher the percentage.}
\label{fig:confusion}
\end{figure}

\begin{table}[tb]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Hit & 259 & Missed & 33 & Mislabel & 7 \\
\hline
Precision & 0.86 & Recall & 0.87 & F1 & 0.86\\
\hline
Avg. $e^\text{start}_i$ & 0.28\% & Avg. $e_i^\text{stop}$ & -3.17\% & ATSR &
0.91
\\
\hline
\multicolumn{5}{|r|} {\textbf{Final Score}} & \textbf{0.871}  \\
\hline
\end{tabular}
\caption{Final test result on unseen data.}
\label{tab:dev-score}
\end{center}
\end{table}

Table~\ref{tab:dev-score} shows the final test result on unseen data based on 
the model trained on all the development data using feature vectors computed
from our salience based hand tracking method.
This result is provided by the ChAirGest organizer after running our
program. The decrease in performance may be due to the overfitting of the model
on the development data.

\section{Conclusions and Future Works}
Our gesture spotting and recognition method based on concatenated HMMs trained
for the three gesture phases and Viterbi decoding gives good results on the
data set. Using hand position features computed from our gesture salience
detection method also helps to increase the gesture recognition accuracy.
Compared with using the hand position from the Kinect SDK, using our hand
tracking method gives a 3.7\% absolute increase in the recognition F1 score.

For future works, we are going to apply discriminative training for learning the
parameters of HMMs. We are also going to compare the HMM based method with the
hidden conditional random field (HCRF) based methods \cite{morency07, wang06}.
It would be interesting to explore whether we can also concatenate the HCRF
models.

For hand tracking, we can apply temporal smoothing to depth data to
see whether that helps to improve accuracy.
 
\vfill
%\end{document}  % This is where a 'short' article might terminate

% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
\vfill\eject
%\balancecolumns % GM June 2007
% That's all folks!
\end{document}
